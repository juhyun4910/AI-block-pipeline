services:
  ui:
    image: python:3.11-slim
    working_dir: /app
    command: sh -c "pip install --no-cache-dir fastapi uvicorn[standard] jinja2 && uvicorn app.main:app --host 0.0.0.0 --port 3000"
    ports:
      - "3000:3000"
    volumes:
      - ./:/app:ro
    networks:
      - ragnet
  api:
    build:
      context: .
      dockerfile: backend/Dockerfile
    env_file: .env
    command: uvicorn backend.app:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
      minio:
        condition: service_healthy
      embedding:
        condition: service_started
    volumes:
      - ./.env:/app/.env:ro
    networks:
      - ragnet

  worker:
    build:
      context: .
      dockerfile: backend/Dockerfile
    env_file: .env
    command: celery -A backend.workers.celery_app worker -l info
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
      embedding:
        condition: service_started
    networks:
      - ragnet

  embedding:
    build:
      context: embedding-svc
    env_file: .env
    command: uvicorn embedding_svc.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8001:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ragnet

  redis:
    image: redis:7.2
    command: redis-server --save "" --appendonly no
    ports:
      - "6379:6379"
    networks:
      - ragnet

  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_DB: postgres
    # 호스트 포트 노출을 제거해 Windows 포트 바인딩 충돌을 회피합니다.
    # 내부 서비스들은 `postgres:5432`로 접근합니다.
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./infra/initdb:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - ragnet

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9000:9000"
      - "9002:9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ragnet

  createbuckets:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: |
      /bin/sh -c "mc alias set local http://minio:9000 minioadmin minioadmin && mc mb -p local/docs || true"
    networks:
      - ragnet

  ollama:
    image: ollama/ollama:0.1.27
    pull_policy: if_not_present
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - ragnet

networks:
  ragnet:
    driver: bridge

volumes:
  pgdata:
  minio-data:
  ollama-data:
